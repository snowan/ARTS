## Week 2 ARTS
------
### [A] - LC: 865. Smallest Subtree with all the Deepest Nodes
------
```java
package leetcode;

import java.util.HashMap;
import java.util.Map;

/**
 * 865. Smallest Subtree with all the Deepest Nodes
 *
 * Given a binary tree rooted at root, the depth of each node is the shortest distance to the root.
 *
 * A node is deepest if it has the largest depth possible among any node in the entire tree.
 *
 * The subtree of a node is that node, plus the set of all descendants of that node.
 *
 * Return the node with the largest depth such that it contains all the deepest nodes in its subtree.
 *
 *
 *
 * Example 1:
 *
 * Input: [3,5,1,6,2,0,8,null,null,7,4]
 * Output: [2,7,4]
 * Explanation:
 *
 *
 *
 * We return the node with value 2, colored in yellow in the diagram.
 * The nodes colored in blue are the deepest nodes of the tree.
 * The input "[3, 5, 1, 6, 2, 0, 8, null, null, 7, 4]" is a serialization of the given tree.
 * The output "[2, 7, 4]" is a serialization of the subtree rooted at the node with value 2.
 * Both the input and output have TreeNode type.
 *
 *
 * Note:
 *
 * The number of nodes in the tree will be between 1 and 500.
 * The values of each node are unique.
 */
public class SmallestSubtree865 {
  class TreeNode {
    int val;
    TreeNode left, right;

    public TreeNode(int val) {
      this.val = val;
    }
  }

//  Solution 1:
//  Post Order to build bottom-up Node, store potential and status when goes up.
//  1. computer the left and right node height, compare the height
//  2. left.height == right.height => potential node is likely the answer.
//  3. left.height > right.height => potential node should point to left node.
//  4. left.height < right.height => potential node is likely be right ndoe.
  public TreeNode subtreeWithAllDeepest(TreeNode root) {
    return helper(root).potential;
  }

  class NodeStatus {
    TreeNode potential;
    int height;
  }

  private NodeStatus helper(TreeNode root) {
    if (root == null) return new NodeStatus();

    NodeStatus leftStatus = helper(root.left);
    NodeStatus rightStatus = helper(root.right);

    NodeStatus status = new NodeStatus();
    status.height = 1 + Math.max(leftStatus.height, rightStatus.height);
    if (leftStatus.height == rightStatus.height) {
      status.potential = root;
    } else if (leftStatus.height > rightStatus.height) {
      status.potential = leftStatus.potential;
    } else {
      status.potential = rightStatus.potential;
    }
    return status;
  }

  // Solution 2: DFS
  // visit every node once, time: O(n)
//  maintain Map<TreeNode, maxChildHeight>
//  recursively dfs: return deepest node with all leaves:
//  1. if left.height == right.height, return parent
//  2. if left.height > right.height, return left
//  3. if left.height < right.height, return right
    public TreeNode subTreeWithDeepest_dfs(TreeNode root) {
      return dfs(root, 0, new HashMap<>());
    }

    private TreeNode dfs(TreeNode node, int height, Map<TreeNode, Integer> map) {
      if (node == null) return node;
      if (node.left == null && node.right == null) {
        map.put(node, height);
        return node;
      }

      TreeNode left = dfs(node.left, height + 1, map);
      TreeNode right = dfs(node.right, height + 1, map);

      if(left == null || right == null) {
        int maxHeight = left == null ? map.get(right) : map.get(left);
        map.put(node, maxHeight);
        return left == null ? right : left;
      }
      if (map.get(left) == map.get(right)) {
        map.put(node, map.get(left));
        return node;
      }
      if (map.get(left) > map.get(right)) {
        map.put(node, map.get(left));
        return left;
      } else {
        map.put(node, map.get(right));
        return right;
      }
    }
}
```
### [R] - [The original Google BigTable paper links](https://static.googleusercontent.com/media/research.google.com/en//archive/bigtable-osdi06.pdf)

Notes:






(This is how BigTable store and look up location informations.)




(BigTable used LSM tree and WAL log to perform read/write operations.(GC ways to merge persistant data files)

*Write Process* 
Write operation -> check well-formedness and ACL(from Chubby file) -> commit log -> memtable -> done

*Read Process*
Read opertion ->  check well-formedness and ACL(from Chubby file) -> merge view of memtable and SS Table -> done

Through the procudure of read/write operations we can see that HBase can have a good write throughput.

### [T] - 

### [S] - [Why Percentiles don't work the way you think](https://www.vividcortex.com/blog/why-percentiles-dont-work-the-way-you-think)

My summary:

- Do not use average (Avg hides outlisers, cannot see them) 

- Outliers skew averages, so in a system with outliers, the average doesn’t represent typical behavior.

- Histogram percentile is wrong. "Percentiles are computed from a population of data, and have to be recalculated every time the population (time interval) changes. Time series databases with traditional metrics don’t have the original population." 
  - E.g. [StatsD issue](https://github.com/etsy/statsd/issues/157)
  
- Approximate percentiles can be computed from histograms, banded metrics, and other useful techniques.

- Regardless of their exact values, percentile metrics tend to a) show outlying behavior and b) get bigger when outlying behavior gets badder. Super useful. 

- Distribution metrics provides correct metrics (global metrics).



