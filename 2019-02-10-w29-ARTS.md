## Week 29 ARTS

### [A] - LC 300, 399, 301, 339, 65
---
```java
package leetcode;

/**
 * 300. Longest Increasing Subsequence
 *
 * Given an unsorted array of integers, find the length of longest increasing subsequence.
 *
 * Example:
 *
 * Input: [10,9,2,5,3,7,101,18]
 * Output: 4
 * Explanation: The longest increasing subsequence is [2,3,7,101], therefore the length is 4.
 * Note:
 *
 * There may be more than one LIS combination, it is only necessary for you to return the length.
 * Your algorithm should run in O(n2) complexity.
 * Follow up: Could you improve it to O(n log n) time complexity?
 */
public class LongestIncreasingSubsequence300 {
  /**
   * Solution: 1. brute force, for each item, any other item has two states, either be subsequence or not,
   * we have 2^n possible subsequences to visit.
   *
   * TC: O(2^n), SC: O(n^2)
   *
   * 2. Dynamic Programming (DP),
   * dp[i] to record ith item of longest size of subsequence
   * for given sub-array, nums[1...m] (1 <= m < len), we assume that we know the size
   * of subsequence for all arrays in nums[1...m] (1 <= m < len), meaning, we know dp[1], dp[2], ... dp[m],
   * now that we add one more item into array nums[m+1], how do we find out dp[m+1],
   * notice that we already know dp[1], dp[2], ... dp[m], so we can find the largest dp[i] (1 <= i < m + 1)
   * which satisfy with condition nums[i] < nums[m+1], then we can get dp[m+1] = nums[i] + 1, otherwise, dp[m+1] = 1
   * so we get equation:
   *
   * dp[i] = max(dp[j]) + 1 (1 <= j < i, and nums[j] < nums[i])
   * or dp[i] = 1 if no such j exsit in dp[1...j].
   *
   * for example: [5,2,4,6,3,7]
   * dp[] = [1,1,1,1,1,1]
   * i = 0, dp = [1,1,1,1,1,1]
   * i = 1, 2 < 5, dp = [1,1,1,1,1,1]
   * i = 3, 4 > 2, currMax = 1, dp = [1, 1, 2, 1,1,1]
   * i = 4, 6 > 4, currMax = 2 , 6 > 5, dp[j] = 1 < currMax, dp = [1,1,2,3,1,1]
   * i = 5, 3 < 6, 3 < 4 continue, 3 > 2, currMax = 1, dp =[1,1,2,3,2]
   * i = 6, 7 > 3, currMax = 2, 7 > 6, currMax = 3, 7 > 4, currMax = 3, dp = [1,1,2,3,2,4]
   *
   * max=4
   *
   * TC: O(n^2), SC: O(n)
   */
  public int lengthOfLIS(int[] nums) {
    if (nums == null) return 0;
    int len = nums.length;
    if (len < 2) return len;
    int[] dp = new int[len];
    int max = 0;
    for (int i = 0; i < len; i++) {
      dp[i] = 1;
      int currMax = 0;
      for (int j = 0; j < i; j++) {
        if (nums[j] < nums[i]) {
          // find the maxisum dp[j]
          currMax = Math.max(currMax, dp[j]);
        }
      }
      dp[i] = currMax + 1;
      max = Math.max(max, dp[i]);
    }
    return max;
  }
}
```
```java
package leetcode;

import java.util.*;

/**
 * 399. Evaluate Division
 *
 * Equations are given in the format A / B = k, where A and B are variables represented as strings,
 * and k is a real number (floating point number). Given some queries, return the answers.
 * If the answer does not exist, return -1.0.
 *
 * Example:
 * Given a / b = 2.0, b / c = 3.0.
 * queries are: a / c = ?, b / a = ?, a / e = ?, a / a = ?, x / x = ? .
 * return [6.0, 0.5, -1.0, 1.0, -1.0 ].
 *
 * The input is: vector<pair<string, string>> equations, vector<double>& values, vector<pair<string, string>> queries ,
 * where equations.size() == values.size(), and the values are positive. This represents the equations. Return vector<double>.
 *
 * According to the example above:
 *
 * equations = [ ["a", "b"], ["b", "c"] ],
 * values = [2.0, 3.0],
 * queries = [ ["a", "c"], ["b", "a"], ["a", "e"], ["a", "a"], ["x", "x"] ].
 * The input is always valid. You may assume that evaluating the queries will result in no division by zero and
 * there is no contradiction.
 */
public class EvaluateDivistion399 {
  /**
   * Solution: can treat this as un-direct graph problem. and using a map of map, each variable as node, map contains
   * all possible node and values from given equation,
   * for example: a/b=2.0, b/c=3.0
   * map:[a: [b, 2.0]]
   *     [b: [a, 0.5],[c, 3.0]]
   *     [c: [b, 1/3.0]]
   *
   * after formatting map, DFS, with given query, we check whether node is in map, and scan the value(map), calculate results
   * for example: query: a/c=?
   * a-> [b, 2.0], a / b * b / c = a * c = 6.0
   *
   */
  public double[] calcEquation(String[][] equations, double[] values, String[][] queries) {
    Map<String, Map<String, Double>> map = new HashMap<>();
    for (int i = 0; i < equations.length; i++) {
      String[] equation = equations[i];
      addEquation2Map(equation[0], equation[1], values[i], map);
      addEquation2Map(equation[1], equation[0], 1 / values[i], map);
    }

    double[] res = new double[queries.length];
    for (int i = 0; i < queries.length; i++) {
      String[] query = queries[i];
      Double currRes = dfs(query[0], query[1], map, new HashSet<>());
      res[i] = (currRes == null ? -1.0 : currRes);
    }

    System.out.println(Arrays.toString(res));
    return res;
  }

  private Double dfs(String num, String deNum, Map<String, Map<String, Double>> map, Set<String> visitedNode) {
    // here using num->denum, because this is undirect graph, a->b and b->a are different equations
    String currEquation = num + "->" + deNum;
    if (visitedNode.contains(currEquation)) {
      return null;
    }
    // map does not contain query node, meaning no possible equation
    if (!map.containsKey(num) || !map.containsKey(deNum)) {
      return null;
    }
    if (num.equals(deNum)) return 1.0;
    Map<String, Double> currMap = map.get(num);
    visitedNode.add(currEquation);
    for (String key : currMap.keySet()) {
      Double res = dfs(key, deNum, map, visitedNode);
      if (res != null) {
        return currMap.get(key) * res;
      }
    }
    visitedNode.remove(currEquation);
    return null;
  }

  private void addEquation2Map(String num, String deNum, double value, Map<String, Map<String, Double>> map) {
    if (!map.containsKey(num)) {
      map.put(num, new HashMap<>());
    }
    map.get(num).put(deNum, value);
  }
}
```
```java
package leetcode;

import java.util.LinkedList;
import java.util.List;
import java.util.Queue;

/**
 * 339. Nested List Weight Sum
 *
 * Given a nested list of integers, return the sum of all integers in the list weighted by their depth.
 *
 * Each element is either an integer, or a list -- whose elements may also be integers or other lists.
 *
 * Example 1:
 *
 * Input: [[1,1],2,[1,1]]
 * Output: 10
 * Explanation: Four 1's at depth 2, one 2 at depth 1.
 * Example 2:
 *
 * Input: [1,[4,[6]]]
 * Output: 27
 * Explanation: One 1 at depth 1, one 4 at depth 2, and one 6 at depth 3; 1 + 4*2 + 6*3 = 27.
 */


  // This is the interface that allows for creating nested lists.
  // You should not implement it, or speculate about its implementation
interface NestedInteger {
    // Constructor initializes an empty nested list.
//    public NestedInteger();

    // Constructor initializes a single integer.
//    public NestedInteger(int value);

    // @return true if this NestedInteger holds a single integer, rather than a nested list.
    public boolean isInteger();

    // @return the single integer that this NestedInteger holds, if it holds a single integer
    // Return null if this NestedInteger holds a nested list
    public Integer getInteger();

    // Set this NestedInteger to hold a single integer.
    public void setInteger(int value);

    // Set this NestedInteger to hold a nested list and adds a nested integer to it.
    public void add(NestedInteger ni);

    // @return the nested list that this NestedInteger holds, if it holds a nested list
    // Return null if this NestedInteger holds a single integer
    public List<NestedInteger> getList();
}

public class NestedListWeightSum339 {
  /**
   * Solution: nested input, recursion will be a good fit.
   * can treat nested input as tree, and using DFS or BFS(layer by layer) to add sum.
   *
   * TC: O(n) - n is the total nested elements.
   * SC: O(m) - m is the number of recursive calls on the stack, here we can say that m would hold the maximum layer(depth)
   * of nest.
   * for example:
   * [[1,1l,2,[1,1]] - would be 2 (2 depth).
   * [[[2,4]],6] - would be 3 (2 depth)
   */
  public int depthSum_DFS(List<NestedInteger> nestedIntegers) {
    if (nestedIntegers == null || nestedIntegers.size() == 0) return 0;
    return helper(nestedIntegers, 1);
  }
  private int helper(List<NestedInteger> nestedIntegers, int depth) {
    int sum = 0;
    for (NestedInteger ni : nestedIntegers) {
      if (ni.isInteger()) {
        sum += ni.getInteger() * depth;
      } else {
        sum += helper(ni.getList(), depth + 1);
      }
    }
    return sum;
  }

  public int depthSum_BFS(List<NestedInteger> nestedIntegers) {
    if (nestedIntegers == null || nestedIntegers.size() == 0) return 0;
    Queue<NestedInteger> queue = new LinkedList<>();
    addList2Queue(nestedIntegers, queue);
    int depth = 0;
    int sum = 0;
    while (!queue.isEmpty()) {
      int size = queue.size();
      depth++;
      while (size-- > 0) {
        NestedInteger curr = queue.poll();
        if (curr.isInteger()) {
          sum += curr.getInteger() * depth;
        } else {
          addList2Queue(curr.getList(), queue);
        }
      }
    }
    return sum;
  }
  private void addList2Queue(List<NestedInteger> nestedIntegers, Queue<NestedInteger> queue) {
    nestedIntegers.forEach(ni -> queue.add(ni));
  }
}
```
```java
package leetcode;


/**
 * 65. Valid Number
 *
 * Validate if a given string can be interpreted as a decimal number.
 *
 * Some examples:
 * "0" => true
 * " 0.1 " => true
 * "abc" => false
 * "1 a" => false
 * "2e10" => true
 * " -90e3   " => true
 * " 1e" => false
 * "e3" => false
 * " 6e-1" => true
 * " 99e2.5 " => false
 * "53.5e93" => true
 * " --6 " => false
 * "-+3" => false
 * "95a54e53" => false
 *
 * Note: It is intended for the problem statement to be ambiguous. You should gather all requirements up front before
 * implementing one. However, here is a list of characters that can be in a valid decimal number:
 *
 * Numbers 0-9
 * Exponent - "e"
 * Positive/negative sign - "+"/"-"
 * Decimal point - "."
 * Of course, the context of these characters also matters in the input.
 *
 * Update (2015-02-10):
 * The signature of the C++ function had been updated. If you still see your function signature accepts a
 * const char * argument, please click the reload button to reset your code definition.
 *
 */
public class ValidNumber65 {
  /**
   * Solution: this problem is intuitive, but there are many corner cases need to be addressed accordingly.
   * following rules:
   * 1. dot cannot appear more than 1
   * 2. dot cannot appear after e
   * 3. e cannot appear more than 2
   * 4. e cannot appear on the first character
   * 5. '-' and '+' can either be the fist character or right after 'e'
   * 6. non number or the symbol '.','e','-','+' not allowed
   *
   * if follow above rules, scan the whole string character by character
   *
   * TC: O(n) - n the length of input string
   * SC: O(1) - no extra memory data structure used
   */
  public boolean isNumber(String s) {
    if (s == null || s.length() < 0) return false;
    boolean dotSeen = false;
    boolean eSeen = false;
    boolean numSeen = false;
    boolean numAfterESeen = true;
    for (int i = 0; i < s.length(); i++) {
      char ch = s.charAt(i);
      if (Character.isDigit(ch)) {
        numAfterESeen = true;
        numSeen = true;
      } else if (ch == '.') {
        if (dotSeen || eSeen) return false;
        dotSeen = true;
      } else if (ch == 'e' || ch == 'E') {
        if (eSeen || !numSeen) return false;
        eSeen = true;
        numAfterESeen = false;
      } else if (ch == '-' || ch == '+') {
        if (i != 0 && s.charAt(i - 1) != 'e') return false;
      } else return false;
    }
    return numSeen && numAfterESeen;
  }
}
```

### [R] - [Spark](https://www.usenix.org/system/files/conference/nsdi12/nsdi12-final138.pdf)
---
# Resilient Distributed Datasets (RDDs)

## Abstraction

Formally, an RDD is a read-only, partitioned collection of records. RDDs can only be created through deterministic operations on either (1) data in stable storage or (2) other RDDs.

RDDs can be reconstructed after a failure, hence it does not need to be materialized all the time. 

Users can use persistence and partitioning feature to indicate RDDs' storage (e.g. in-memory) and partitioning (e.g. key based harsh) strategy.

## Programming Interface

- transformation: start by defining one or more RDDs through transformations on data in stable storage (e.g. map and filter)
- actions: operations that return a value to the application or export data to a storage system. (like *count, collect, save*)
- persist: programmers can call a persist method to indicate which RDDs they want to reuse in future operations. Spark keeps persistent RDDs in memory by default, but it can spill them to disk if there is not enough RAM.

Spark computes RDDs lazily the ﬁrst time they are used in an action, so that it can pipeline transformations.

## Advantages

- The main difference between RDDs and DSM(Distributed Shared Memory) is that RDDs can only be created (“written”) through coarse-grained transformations, while DSM allows reads and writes to each memory location. This limits RDDs' bulk write but allows for efficient fault tolerance (no overhead of checkpoints and may only recalculation of partial RDDs)
- The immutable nature lets a system mitigate slow nodes (stragglers) by running backup copies of slow tasks as in MapReduce.
- in bulk operations on RDDs, a runtime can schedule tasks based on data locality to improve performance
- RDDs degrade gracefully when there is no enough memory to store them, as long as they are only being used in scan-based operations. Partitions that do not ﬁt in RAM can be stored on disk and will provide similar performance to current data-parallel systems.

![alt text](/images/spark_RDD.png)

# Programming Interface Details

![alt text](/images/spark_runtime.png)


*Driver* program connects to a cluster of workers which deﬁnes one or more RDDs and invokes actions on them. Spark code on the driver also tracks the RDDs’ lineage. The workers are long-lived processes that can store RDD partitions in RAM across operations.

![alt text](/images/spark_tx.png)

Table 2 lists the main RDD transformations and actions available in Spark. ***Transformations*** are lazy operations that deﬁne a new RDD, while ***actions*** launch a computation to return a value to the program or write data to external storage.

# Representing RDDs

![alt text](/images/spark_interface.png)

Spark represents each RDD through a common interface that exposes ﬁve pieces of information:

1. a set of partitions, which are atomic pieces of the dataset;
2. a set of dependencies on parent RDDs; 
3. a function for computing the dataset based on its parents;
4. metadata about its partitioning scheme and data placement.

Two types to classify dependencies between RDDs:

1. *narrow dependencies*, where each partition of the parent RDD is used by at most one partition of the child RDD
2. *wide dependencies*, where multiple child partitions may depend on it.

![alt text](/images/spark_RDD_example.png)

Benefits of these two types of dependencies:

1. narrow dependencies allow for pipelined execution on one cluster node, which can compute all the parent partitions. In contrast, wide dependencies require data from all parent partitions to be available and to be shufﬂed across the nodes using a MapReduce like operation.
2. Recovery after a node failure is more efﬁcient with a narrow dependency, as only the lost parent partitions need to be recomputed, and they can be recomputed in parallel on different nodes. In contrast, in a lineage graph with wide dependencies, a single failed node might cause the loss of some partition from all the ancestors of an RDD, requiring a complete re-execution.

# Implementation

## Job Scheduling

![alt text](/images/spark_job_schedule.png)
- Whenever a user runs an action (e.g., count or save) on an RDD, the scheduler examines that RDD’s lineage graph to build a DAG of stages to execute. Each stage contains as many pipelined transformations with narrow dependencies as possible. The boundaries of the stages are the shufﬂe operations required for wide dependencies. The scheduler then launches tasks to compute missing partitions from each stage until it has computed the target RDD.
- Scheduler assigns tasks to machines based on data locality.
- For wide dependencies, scheduler materialize intermediate records on some nodes like MapReduce materializes map outputs.
- If tasks fail, rerun another on another node if parents are still available, otherwise resubmit tasks to compute missing partitions in parallel.
- Finally, although all computations in Spark currently run in response to actions called in the driver program, spark is experimenting with letting tasks on the cluster (e.g., maps) call the lookup operation, which provides random access to elements of hash-partitioned RDDs by key. In this case, tasks would need to tell the scheduler to compute the required partition if it is missing.

## Memory Management

Three options of storage:

1. in-memory storage as deserialized Java objects (fastest as JVM can directly access RDD natively)
2. in-memory storage as serialized data (memory-efficient with lower performance compared to 1.)
3. on-disk storage

### [T] - [App, all in one](https://feedly.com/i/welcome)
---
All in one places, read all news feed in one app.

### [S] - [Continues MySQL backup validation: Restoring backups](https://code.fb.com/data-infrastructure/continuous-mysql-backup-validation-restoring-backups/)
---
A system which continuously test ability to restore data(MySQL) from backups. 

Two main Components in System:
- **Continous Restore Tier (CRT)** - All schedule and mornitor restores. It looks for databases with new backups and creates restore jobs for them, monitors the restore’s progress, and ensures each backup is successfully restored.
- **ORC Restore Coordinator (ORC)** - Comprises the restore workers (peons) and a load balancer (warchief). The warchief receives new restore jobs from CRT and assigns them to peons. Peons host a local MySQL instance where they perform the actual restore.

**Backup types:**
1. **Full logical backups** - taken every few days using `mysqldump`
2. **Differential(Diff) backups** - taken on days, no full backups. restore the difference between current full dump and last full dump. 
3. **Binary log(binlog) backups** - constantly streamed to HDFS from the database master.

## ORC Restore Coordinator

### Architecture
![alt text](/images/fb_mysql-restore.png)

From pic, we can see ORC contains 3 main components:
1. **Warchief** - load balancer
2. **ORC DB** – Central MySQL database that maintains state about jobs assigned to each peon, current state of each job, peon health stats, and more. The information stored here is used by the warchief to decide which peon a job should be assigned to, as well as by peons during crash recovery.
3. **Peon** - The restore workers.

### Peon
Peons contain all relevant logic for retrieving backups from HDFS, loading them into their local MySQL instance, and rolling them forward to a certain point in time by replaying binlogs. Each restore job a peon works on goes through these five stages:

1. **SELECT** – Decide which backup needs to be restored for this shard (i.e. full or diff, HDFS or offsite, etc.)
2. **DOWNLOAD** – Download the selected files to disk. 
3. **LOAD** – Load the downloaded backup into the peon’s local MySQL instance. 
4. **VERIFY** – Perform sanity checks on the data loaded into MySQL.
5. **REPLAY** – Download and replay transactions from binary log backups on top of the restored backup, if required.

